package climaCluster;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import org.apache.spark.mllib.clustering.KMeansModel;
import org.apache.spark.mllib.clustering.KMeans;
import org.apache.spark.mllib.linalg.Vector;
import org.apache.spark.mllib.linalg.Vectors;
public class KMeansClimate {
	
	public static void main(String[] args) {
		
		List<String> reqVars  = Arrays.asList("t", "ws", "rh");
		SparkConf conf = new SparkConf().setMaster("local").setAppName("App");
	    //val conf = new SparkConf().setMaster("local").setAppName("myApp")
	    JavaSparkContext jsc = new JavaSparkContext(conf);
	    
	    String path = args[0];
	    JavaRDD<String> data = jsc.textFile(path);
	    List<String> head = data.take(10);
	    JavaRDD<Vector> parsedData = data               // convert list to stream
                .filter(line -> !isHeader(line))    //filter out header with function
                .map(s->getValues(s,reqVars));
                //.collect(Collectors.toList());
	    
	    parsedData.cache();
	    for (Vector line: parsedData.take(10)) {
	    	System.out.println(line);
	    	}
	    
	    //result.forEach(item->System.out.println(item));
	    //format of csv is 
	    //latitude,longitude,yr,month,temp,vp,rh,tmin,tmax,trange,precip,windSpd,temp,rh,precip
	    //parse data to give vectors of temp, rh and wind
	    
	    
		    
		 // Cluster the data into two classes using KMeans
		    int numClusters = 2;
		    int numIterations = 20;
		    KMeansModel clusters = KMeans.train(parsedData.rdd(), numClusters, numIterations);

		    System.out.println("Cluster centers:");
		    for (Vector center: clusters.clusterCenters()) {
		      System.out.println(" " + center);
		    }
		    double cost = clusters.computeCost(parsedData.rdd());
		    System.out.println("Cost: " + cost);

		    // Evaluate clustering by computing Within Set Sum of Squared Errors
		    double WSSSE = clusters.computeCost(parsedData.rdd());
		    System.out.println("Within Set Sum of Squared Errors = " + WSSSE);

		    // Save and load model
		    clusters.save(jsc.sc(), "target/org/apache/spark/JavaKMeansExample/KMeansModel");
		    KMeansModel sameModel = KMeansModel.load(jsc.sc(),
		      "target/org/apache/spark/JavaKMeansExample/KMeansModel");
	    jsc.stop();

	}
	private static boolean isHeader(String line)
	{
		return line.contains("latitude");
	}
	private static Vector getValues(String line,List<String> reqVariables)
	{
	      String[] sarray = line.split(",");
	      double[] values = new double[reqVariables.size()];
	      for(int i=0;i<reqVariables.size();i++)
	      {
	    	  String var = reqVariables.get(i);
	    	  switch(var)
	    	  {
	    	  case "t":
	    		  values[i] = Double.parseDouble(sarray[4]);
	    		  break;
	    	  case "vp":
	    		  values[i] = Double.parseDouble(sarray[5]);
	    		  break;
	    	  case "rh":
	    		  values[i] = Double.parseDouble(sarray[6]);
	    		  break;
	    	  case "tmin":
	    		  values[i] = Double.parseDouble(sarray[7]);
	    		  break;
	    	  case "tmax":
	    		  values[i] = Double.parseDouble(sarray[8]);
	    		  break;
	    	  case "trange":
	    		  values[i] = Double.parseDouble(sarray[9]);
	    		  break;
	    	  case "pr":
	    		  values[i] = Double.parseDouble(sarray[10]);
	    		  break;
	    	  case "ws":
	    		  values[i] = Double.parseDouble(sarray[11]);
	    		  break;
	    	  }
	      }
	      
	      return Vectors.dense(values);
    }
}
